Sender: LSF System <lsfadmin@host290.jc.rl.ac.uk>
Subject: Job 9389057: <# MPI myname ;#BSUB -q  par-single ;#BSUB -n 16;#BSUB -W 00:2;#BSUB -o %J.lo ;#BSUB -e %J.err  ; # Load any environment modules (needed for mpi_myname.exe);module load libfftw/intel/3.2.2_mpi; # Submit the job using;mpirun.lotus ./a.out> in cluster <lotus> Done

Job <# MPI myname ;#BSUB -q  par-single ;#BSUB -n 16;#BSUB -W 00:2;#BSUB -o %J.lo ;#BSUB -e %J.err  ; # Load any environment modules (needed for mpi_myname.exe);module load libfftw/intel/3.2.2_mpi; # Submit the job using;mpirun.lotus ./a.out> was submitted from host <jasmin-sci1-panfs.ceda.ac.uk> by user <stefstef> in cluster <lotus> at Tue Mar  5 16:27:51 2019.
Job was executed on host(s) <16*host290.jc.rl.ac.uk>, in queue <par-single>, as user <stefstef> in cluster <lotus> at Tue Mar  5 16:40:16 2019.
</home/users/stefstef> was used as the home directory.
</home/users/stefstef> was used as the working directory.
Started at Tue Mar  5 16:40:16 2019.
Terminated at Tue Mar  5 16:40:19 2019.
Results reported at Tue Mar  5 16:40:19 2019.

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
# MPI myname 
#BSUB -q  par-single 
#BSUB -n 16
#BSUB -W 00:2
#BSUB -o %J.lo 
#BSUB -e %J.err  

# Load any environment modules (needed for mpi_myname.exe)
module load libfftw/intel/3.2.2_mpi

# Submit the job using
mpirun.lotus ./a.out

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   10.70 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   2 sec.
    Turnaround time :                            748 sec.

The output (if any) follows:

Parsing application description...
Identifying hosts...
Spawning processes...
Process layout for world 0 is as follows:
mpirun:  proc 27236
  daemon proc 27240 on host 192.168.120.190
    rank 0:  proc 27244
    rank 1:  proc 27245
    rank 2:  proc 27246
    rank 3:  proc 27247
    rank 4:  proc 27248
    rank 5:  proc 27249
    rank 6:  proc 27250
    rank 7:  proc 27251
    rank 8:  proc 27252
    rank 9:  proc 27253
    rank 10:  proc 27254
    rank 11:  proc 27255
    rank 12:  proc 27256
    rank 13:  proc 27257
    rank 14:  proc 27258
    rank 15:  proc 27259
Host 0 -- ip 192.168.120.190 -- ranks 0 - 15

 host | 0
======|======
    0 : SHM

 Prot -  All Intra-node communication is: SHM

Size of cluster 16
host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk

host290.jc.rl.ac.uk



PS:

Read file <9389057.err> for stderr output of this job.

